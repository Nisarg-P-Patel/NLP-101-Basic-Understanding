# -*- coding: utf-8 -*-
"""NLP basics with NLTK.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TgMc9fJ-m5E-hWcqpK-qYXRn264E_YqA
"""

# Importing necessary libraries

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import pandas as pd
import matplotlib.pyplot as plt

# making documents(strings) to perform pre-processing

document1='Hello,How are you?'
document2='I"m fine and what about you?'
document3='All good. I"m glad to know you are also fine.'
document4='I"m always fine and fit. By the way, how are you doing in your academics?'
document5='Not stable as I"m also preparing for Master"s.'

document1='The Natural Language Toolkit (NLTK) is a platform used for building Python programs that work with human language data for applying in statistical natural language processing (NLP).It contains text processing libraries for tokenization, parsing, classification, stemming, tagging and semantic reasoning.'
document2='It also includes graphical demonstrations and sample data sets as well as accompanied by a cook book and a book which explains the principles behind the underlying language processing tasks that NLTK supports.'
document3='The Natural Language Toolkit is an open source library for the Python programming language originally written by Steven Bird, Edward Loper and Ewan Klein for use in development and education.'
document4='It comes with a hands-on guide that introduces topics in computational linguistics as well as programming fundamentals for Python which makes it suitable for linguists who have no deep knowledge in programming, engineers and researchers that need to delve into computational linguistics, students and educators.'
document5='NLTK includes more than 50 corpora and lexical sources such as the Penn Treebank Corpus, Open Multilingual Wordnet, Problem Report Corpus, and Linâ€™s Dependency Thesaurus.'

# tokenize the documents

tokenized_document=[]
for x in range(1,6):
    tokenized_document.append(list(set(nltk.word_tokenize(eval('document{}'.format(x))))))

print(tokenized_document)

# lowercase conversion of tokens

for x in range(0,5):
    tokenized_document[x]=[y.lower() for y in tokenized_document[x]]

print(tokenized_document)

# removing stop words from documents

stop_words=stopwords.words('english')
stop_words.extend([',','?','""',"''",'.'])
# print(stop_words)

for x in stop_words:
    for y in range(0,5):
        if x in tokenized_document[y]:
            tokenized_document[y].remove(x)

print(tokenized_document)

# performing stemming after removing stop words

stemmed_document=tokenized_document
ps = PorterStemmer()
for x in range(0,5):
    stemmed_document[x]=[ps.stem(y) for y in tokenized_document[x]]

print(stemmed_document)

# collecting distinct words in the saperate list

total_words=[]
for x in range(0,5):
    temp=[y for y in stemmed_document[x]]
    total_words.extend(temp)

total_words=list(set(total_words))

print(total_words)

# making matrix of word-document relation

relation=[]
print("\t\t",end=" ")
for x in total_words:
    print(x+" ",end="\t")
print()
for x in range(0,5):
    print('document{}'.format(x+1),end="\t")
    temp=[]
    for element in total_words:
        if element in stemmed_document[x]:
            temp.extend([1])
            print("1",end="\t")
        else:
            temp.extend([0])
            print("0",end="\t")
    print()
    relation.append(temp)

# prinr(relation)

# Represent term frequencies for each unique words

freq=total_words[:]
for x in range(len(total_words)):
    freq[x]=[row[x] for row in relation].count(1)
# print(freq)

document_count=[]
for x in relation:
    document_count.extend([x.count(1)])
print(document_count)

for x in range(len(total_words)):
        print(total_words[x]," ",str(freq[x]),end="\t")

# visualize the data using histogram
# Plot word frequencies

freq_series = pd.Series(freq)

y_labels = total_words[:]

# Plot the figure.
plt.figure(figsize=(12, 8))
ax = freq_series.plot(kind='barh')
ax.set_title('Words - Frequency')
ax.set_xlabel('Frequency')
ax.set_ylabel('Words')
ax.set_yticklabels(y_labels)
ax.set_xlim(min(freq)-2,max(freq)+1) # expand xlim to make labels easier to read

rects = ax.patches

# visualize the documnet using histogram

freq_series = pd.Series(document_count)

y_labels = [('document{}'.format(x)) for x in range(1,6)]

# Plot the figure.
# plt.figure(figsize=(12, 8))
ax = freq_series.plot(kind='barh')
ax.set_title('document - Frequency')
ax.set_xlabel('Frequency')
ax.set_ylabel('documnet')
ax.set_yticklabels(y_labels)
ax.set_xlim(min(document_count)-2,max(document_count)+2) # expand xlim to make labels easier to read

rects = ax.patches

