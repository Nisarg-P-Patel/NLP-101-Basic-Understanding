# -*- coding: utf-8 -*-
"""Text Classification With LSTM Example2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I7zFDfg1YIGH-9rse7RcIDsJlqtmI8RB

#Downloading Dataset from kaggle.
"""

!wget "https://storage.googleapis.com/kaggle-data-sets/64890/127736/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20210319%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210319T084356Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=49d1f63f25a310969f0e218d9ded155dd2de48cbf0a5258b5f5203b4266dfd358343933ba78ae67cd55a0cc70ec0fd411c0c5e97ee1d613655b3de022034cf1d84e584e066a524744dcfa02654c036082384ba5d1f708620a2d33d19bbc9b08e417359353b39687d4e156ec528af0ec8c5c55d1c4d7be96492ed8f3af10111edb006d4c5b7b48eb39eaee289d8f7a9c621b48ae95cfa9d9024274725a9a02126849a1ca015a1928917651304ebd77b51adec66cb275f5386ad495d102d1a27895a136fc2a80f9aa22d177c3610f6bfb97564e3fe9f419a52f76a71925b3ef695b51e21476a8daf4c589d39efa0b00d89817e584cd3201ef00588b2eca8bd45c7"

!unzip "archive.zip"

"""#Importing Necessary Libraries"""

import pandas as pd
import numpy as np
import nltk
from matplotlib import pyplot as plt
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout
import tensorflow as tf
from tensorflow.keras import Sequential
nltk.download('stopwords')
nltk.download('punkt')

"""#Data exploration and pre-processing"""

df = pd.read_csv("wiki_movie_plots_deduped.csv")
df.head()

del [df['Release Year'],df['Origin/Ethnicity'],df['Director'],df['Cast'],df['Wiki Page']]

df.head()

df['Genre'].value_counts()

requires = {'drama':0,'comedy':1,'horror':2,'action':3,'romance':4}
df = df[df['Genre'].isin(requires)]
df.head()

df['Genre'].value_counts()

print('Size of dataset after filtering the dataset :-',len(df))

df['Genre'] = df['Genre'].map(requires)

df.head()

def toLower(sentence):
    return sentence.lower()

def remove_punctuation(line):
    punc = '''!()-[];:'"\,{}<>./?@#$%^&*_~'''
    digit = []
    for i in range(0,10):
        digit.append(str(i))
    for ele in line:
        if ele in punc or ele in digit:
            line = line.replace(ele, "")
    return line

def tokenizer(sentence):
    tokens = nltk.word_tokenize(sentence)
    return tokens

def stopwords_removal(tokens):
    stop_words = nltk.corpus.stopwords.words('english')
    filtered_tokens = [i for i in tokens if not i in stop_words]
    return filtered_tokens

def stemming(tokens):
    stemmer = nltk.stem.porter.PorterStemmer()
    stemmed_tokens = [stemmer.stem(i) for i in tokens]
    return stemmed_tokens

def tokenize_and_stem(text):
    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token
     # filter out any tokens not containing letters (e.g., raw punctuation)
    text = remove_punctuation(text)
    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]
    tokens = stopwords_removal(tokens)
    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)
    stems = stemming(tokens)
    return ' '.join(stems)

df['Plot']=df['Plot'].apply(tokenize_and_stem)

df.head()

tokenizer = tf.keras.preprocessing.text.Tokenizer(
    filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',
    lower=True, split=' ', char_level=False, oov_token='<oov>',
)

tokenizer.fit_on_texts(list(df['Plot']))

sequences = tokenizer.texts_to_sequences(list(df['Plot']))
max=np.max([len(sequence) for sequence in sequences])
print("Maximum length of sentence is :- ",max)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

data = tf.keras.preprocessing.sequence.pad_sequences(
    sequences, maxlen=max, dtype='int32', padding='pre',
    truncating='pre', value=0.0
)

print(data.shape)

labels = tf.keras.utils.to_categorical(df['Genre'])

"""#Downloading glove embedding file consist of 100 dimesion vectors for each word"""

!wget "https://storage.googleapis.com/kaggle-data-sets/715814/1246668/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20210319%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210319T084754Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=3df5d08fa90734eb9c03c3f52efd61477599fd314b1d86e0be52ccad4d17aefdd31e7508799b9790925bf44c49510d14c0186cbf054967cc748e5723782f85569f52520742986033193387ff3a2d5a1a840350b224d47e2e72738e49780ace8535914f2c01f834b715dae9f564aa5b2e33cf0711ade9c46c1a14e1ddb044f45ec5ab131955a47bf2684ba284bd6ccd3a4e50f22d5971c045e4beb720a155c9e3890a979d4c84fa545915a6ff9bd0375c7f2d36bdc4cfa6d7b64feca5fe1443ed93a027f191d3703789bf4660e3123ca11a15b9ab6b5512d55f0f16191443e9459e7ce4a611f94a115ee070e64abea077a15ea5ceb8071341d5470c4359773c55"

!unzip "archive.zip"

embeddings={}
index=0
with open('glove.6B.100d.txt') as file:
    for embeddingLine in file:
        lineSplit=embeddingLine.split()
        coefs = np.asarray(lineSplit[1:], dtype='float32')
        embeddings[lineSplit[0]]=coefs
        index+=1

print(len(embeddings['hello']))

embeddings_matrix=np.zeros((len(word_index)+1,len(embeddings['a'])))
for word,i in word_index.items():
    if word in embeddings:
        embeddings_matrix[i]=embeddings[word]

X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)

len(requires)

"""#Building and training model"""

model=Sequential()
model.add(Embedding(len(word_index) + 1,
                            len(embeddings['a']),
                            weights=[embeddings_matrix],
                            input_length=max,
                            trainable=False))
model.add(LSTM(256,return_sequences=False))
model.add(Dropout(.5))
model.add(Dense(1000,activation='relu'))
model.add(Dense(len(requires),activation='softmax'))

model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

history = model.fit(X_train,y_train,batch_size=64,epochs=50,validation_data=(X_test,y_test))

from matplotlib import pyplot as plt
plt.plot(range(50),history.history['accuracy'],color='r',label='Train')
plt.plot(range(50),history.history['val_accuracy'],color='b',label='Valid')
plt.legend()
plt.show()

from matplotlib import pyplot as plt
plt.plot(range(50),history.history['loss'],color='r',label='Train')
plt.plot(range(50),history.history['val_loss'],color='b',label='Valid')
plt.legend()
plt.show()

remap = {0:'drama',1:'comedy',2:'horror',3:'action',4:'romance'}

def query_result(sentence):
  sentence = tokenize_and_stem(sentence)
  sequenced = tokenizer.texts_to_sequences([sentence])
  padd = tf.keras.preprocessing.sequence.pad_sequences(
    sequenced, maxlen=max, dtype='int32', padding='pre',
    truncating='pre', value=0.0)
  label = remap[np.argmax(model.predict(padd))]
  return label

query_result(df['Plot'].iloc[10])

remap[df['Genre'].iloc[10]]

"""#Checking model with input of avengers movie plot which is an action movie."""

sentence = input()

query_result(sentence)