# -*- coding: utf-8 -*-
"""Text Classification with LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19TjN0z0SAsed3LodNYPHJtMggZQ2ZI8J
"""

# kaggle data: https://www.kaggle.com/datasets/jbencina/department-of-justice-20092018-press-releases

import pandas as pd
import numpy as np
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import LSTM,Dense,Dropout,Embedding
import tensorflow as tf
from tensorflow.keras import Sequential
nltk.download('stopwords')
nltk.download('punkt')

from google.colab import drive
drive.mount('/content/drive')

add='/content/drive/My Drive/Colab Notebooks/Department of Justice 2009-2018 Press Releases.json'
df = pd.read_json(add, lines=True)
df.head()

df.info()

df['components']=df['components'].astype('string')
df.head()

df.info()

df['components'].value_counts()

df['components'].value_counts()[:10]

x=["['Criminal Division']","['Tax Division']","['Civil Division']","['Civil Division']","['Civil Rights Division']","['Antitrust Division']","['Environment and Natural Resources Division']","['Civil Rights Division, Civil Rights - Criminal Section']","['National Security Division (NSD)']","['Civil Rights Division, Civil Rights - Disability Rights Section']"]
index_names = df[(df['components']!=x[0]) & (df['components']!=x[1]) & (df['components']!=x[2]) & (df['components']!=x[3]) & (df['components']!=x[4]) &
                  (df['components']!=x[5]) & (df['components']!=x[6]) & (df['components']!=x[7]) & (df['components']!=x[8]) & (df['components']!=x[9]) ].index
print(len(index_names))
df.drop(index_names, inplace = True)
df.head()

requires = {"['Criminal Division']":1,"['Tax Division']":2,"['Civil Division']":3,"['Civil Division']":4,"['Civil Rights Division']":5,"['Antitrust Division']":6,"['Environment and Natural Resources Division']":7,"['Civil Rights Division, Civil Rights - Criminal Section']":8,"['National Security Division (NSD)']":9,"['Civil Rights Division, Civil Rights - Disability Rights Section']":10}
df['components']=df['components'].replace(requires)
df.head()

def toLower(sentence):
    return sentence.lower()

def tokenizer(sentence):
    tokens = list(set(nltk.word_tokenize(sentence)))
    return tokens

def stopwords_removal(tokens):
    stop_words = nltk.corpus.stopwords.words('english')
    stop_words.extend([',','?','""',"''",'.','!', "'",'"',"'d","'ll",'[',']','--',':',';','///','@', '``',
                       '#', '$', '%', '&', "'re", "'s", '(', ')', '*', '**', '**the', '-', '/', '//',
                       '§', '§§','...','–', '—', '‘', '’', '“', '”', '•', '─',"'m", "'ve", '***'])
    filtered_tokens = [i for i in tokens if not i in stop_words]
    return filtered_tokens

def stemming(tokens):
    stemmer = nltk.stem.porter.PorterStemmer()
    stemmed_tokens = [stemmer.stem(i) for i in tokens]
    return stemmed_tokens

def pre_process(text):
    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]
    tokens = stopwords_removal(tokens)
    stems = stemming(tokens)
    return stems

df['contents']=df['contents'].apply(pre_process)
df['contents']

df.reset_index(inplace = True)
del ( df['index'])

df

tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',lower=True, split=' ', char_level=False, oov_token='<oov>',)
tokenizer.fit_on_texts(list(df['contents']))

sequences = tokenizer.texts_to_sequences(list(df['contents']))
max=np.max([len(sequence) for sequence in sequences])
print("Longest sentence length :- ",max)
word_index = tokenizer.word_index
print('Number of  unique tokens :- %s' % len(word_index))

data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max, dtype='int32', padding='pre',truncating='pre', value=0.0)
print(data.shape)

labels = tf.keras.utils.to_categorical(df['components'])

X_train, X_test, Y_train, Y_test = train_test_split(data, labels, test_size=0.25, random_state=14)

model=Sequential()
model.add(Embedding(len(word_index)+1,64,input_length=max))
model.add(LSTM(256,return_sequences=False))
model.add(Dropout(0.25))
model.add(Dense(1024,activation='relu'))
model.add(Dense(10,activation='softmax'))

model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

model.fit(X_train,Y_train,batch_size=128,epochs=35,validation_data=(X_test,Y_test))

