# -*- coding: utf-8 -*-
"""Text Classification for Sentiment Analysis with NEMO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kaauFhN-Y29BhGBnHtuPVi4gySOjc13m
"""

BRANCH = 'r1.0.0rc1'
!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[nlp]

from nemo.collections import nlp as nemo_nlp
from nemo.utils.exp_manager import exp_manager

import os
import wget
import torch
import pytorch_lightning as pl
from omegaconf import OmegaConf

TASK = 'IMDB'
DATA_DIR = os.path.join(os.getcwd(), 'DATA_DIR')
RE_DATA_DIR = os.path.join(DATA_DIR, 'RE')
WORK_DIR = os.path.join(os.getcwd(), 'WORK_DIR')
MODEL_CONFIG = 'text_classification_config.yaml'

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(os.path.join(DATA_DIR, 'RE'), exist_ok=True)
os.makedirs(WORK_DIR, exist_ok=True)

# download the dataset
wget.download('https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',
              os.path.join(DATA_DIR, 'data_re.tar'))

!tar -xvf  {DATA_DIR}/data_re.tar -C {RE_DATA_DIR}

!rm '/content/DATA_DIR/data_re.tar'

! ls -l $RE_DATA_DIR

RE_DATA_DIR = '/content/DATA_DIR/RE/aclImdb'

wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/text_classification/data/import_datasets.py')

! python import_datasets.py --dataset_name=imdb --source_data_dir={RE_DATA_DIR} --target_data_dir={RE_DATA_DIR}

# let's take a look at the training data
! head -n 5 {RE_DATA_DIR}/train.tsv

# let's check the label mapping
! cat {RE_DATA_DIR}/label_ids.tsv

# download the model's configuration file
config_dir = WORK_DIR + '/configs/'
os.makedirs(config_dir, exist_ok=True)
if not os.path.exists(config_dir + MODEL_CONFIG):
    print('Downloading config file...')
    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/text_classification/conf/' + MODEL_CONFIG, config_dir)
else:
    print ('config file is already exists')

# this line will print the entire config of the model
config_path = f'{WORK_DIR}/configs/{MODEL_CONFIG}'
print(config_path)
config = OmegaConf.load(config_path)

config.model.train_ds.file_path = os.path.join(RE_DATA_DIR, 'train.tsv')
config.model.validation_ds.file_path = os.path.join(RE_DATA_DIR, 'test.tsv')
config.model.test_ds.file_path = os.path.join(RE_DATA_DIR, 'test.tsv')
config.trainer.max_epochs = 1
# Note: these are small batch-sizes - increase as appropriate to available GPU capacity
config.model.train_ds.batch_size=8
config.model.validation_ds.batch_size=8
config.model.dataset.num_classes=2

print(OmegaConf.to_yaml(config))

config.model.task_name = TASK
config.model.output_dir = WORK_DIR
config.model.dataset.data_dir = RE_DATA_DIR

print("Trainer config - \n")
print(OmegaConf.to_yaml(config.trainer))

# lets modify some trainer configs
# checks if we have GPU available and uses it
cuda = 1 if torch.cuda.is_available() else 0
config.trainer.gpus = cuda

# for PyTorch Native AMP set precision=16
config.trainer.precision = 16 if torch.cuda.is_available() else 32

# remove distributed training flags
config.trainer.accelerator = 'DDP'

trainer = pl.Trainer(**config.trainer)

config.exp_manager.exp_dir = WORK_DIR
exp_dir = exp_manager(trainer, config.get("exp_manager", None))

# the exp_dir provides a path to the current experiment for easy access
exp_dir = str(exp_dir)
exp_dir

# complete list of supported BERT-like models
print(nemo_nlp.modules.get_pretrained_lm_models_list())

# specify BERT-like model, you want to use, for example, "megatron-bert-345m-uncased" or 'bert-base-uncased'
PRETRAINED_BERT_MODEL = "biomegatron-bert-345m-uncased"

# add the specified above model parameters to the config
config.model.language_model.pretrained_model_name = PRETRAINED_BERT_MODEL

model = nemo_nlp.models.TextClassificationModel(cfg=config.model, trainer=trainer)

print(config.model.nemo_path)

model.save_to(config.model.nemo_path)

!cp '/content/text_classification_model.nemo' '/content/WORK_DIR/TextClassification/2021-04-17_09-07-24'

# !rm '/content/text_classification_model.nemo'

# Commented out IPython magic to ensure Python compatibility.
try:
    from google import colab
    COLAB_ENV = True
except (ImportError, ModuleNotFoundError):
    COLAB_ENV = False

# Load the TensorBoard notebook extension
if COLAB_ENV:
#     %load_ext tensorboard
#     %tensorboard --logdir {exp_dir}
else:
    print("To use tensorboard, please use this notebook in a Google Colab environment.")

# import torch
# torch.cuda.empty_cache()

# start model training
trainer.fit(model)

model.setup_test_data(test_data_config=config.model.test_ds)

trainer.test(model)

